[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A place to share updates of my work on statistical methodology in clinical trials."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Variance estimation: ANCOVA RCT analysis (part 2)\n\n\n\n\n\n\n\nRCT\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2025\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nVariance estimation: ANCOVA RCT analysis\n\n\n\n\n\n\n\nRCT\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2025\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nCausal interpretation of the hazard ratio in RCTs\n\n\n\n\n\n\n\nCausal inference\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nSimulate from a Normal-inverse-Wishart distribution\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nCovariate adjustment in RCTs: should we condition on baseline observations?\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nStratified or covariate adjusted Cox model?\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nOld blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/causal_hr/causal_hr.html",
    "href": "posts/causal_hr/causal_hr.html",
    "title": "Causal interpretation of the hazard ratio in RCTs",
    "section": "",
    "text": "Motivation\nA new set of articles on the “Causal interpretation of the hazard ratio in randomized clinical trials” ( original article, commentary, response) has pushed me to write this blogpost. It’s something I’ve been considering for a while, not because it’s the most important thing in the world, but there seems to be a persistent confusion, despite some great attempts to clarify\n\n\nSource of the confusion\nThe mistake (in my eyes) is to claim something like “the hazard ratio is not a causal effect” (in the context of an RCT) without first providing a precise definition of “causal effect”.\n\n\nDefinition A\nOne clean and precise definition of a causal effect is provided by Hernan & Robins Technical Point 1.1…\nIn general, a population causal effect can be defined as a contrast of any functional of the marginal distributions of counterfactual outcomes under different actions or treatment values.\nThe hazard ratio, \\(hr(t)\\), satisfies this definition, for any choice of \\(t\\). Therefore, if we follow this definition, then the hazard ratio is a causal effect. End of story. We can all move on.\n\n\nCriticism of hazard ratios in loose language\n“The hazards of hazard ratios” (Hernan, 2010) contains an example where the hazard ratio (hormone therapy vs control) after 5 years is less than 1 “even if hormone therapy has no truly preventive effect in any woman at any time.”\nInterestingly, though, the fact that a hazard ratio can ostensibly point in favour of an experimental treatment despite no true benefit is not the grounds given for claiming that a hazard ratio lacks a causal interpretation. For example, the so-called “total effect” in a competing risks setting also has the property that it can be less than 1 (if it’s a ratio) despite no true benefit. Yet this is unequivocally a causal effect according to the authors.\nRather, the grounds given for claiming the hazard ratio is not a causal effect is always given in loose language, e.g., it somehow does not compare equal groups, or has a so-called “in-built selection bias”. This is ambiguous. I would say that a hazard ratio at 5 years is a consequence of everything that happens to everyone in the first 5 years. It is not merely a comparison of the set of patients alive at 5 years, as if the patients who have died had never existed.\n\n\nDefinition B\nI’ve not seen a precise definition of a causal effect that would rule out the hazard ratio, so I’ll make one up here…\nIn general, a population causal effect can be defined as a contrast of any functional (except for a conditional expectation where the conditioning is on a post-baseline event) of the marginal distributions of counterfactual outcomes under different actions or treatment values.\nIf we follow this definition, then the hazard ratio is not (in general) a causal effect. End of story. We can all move on.\nThe reason for “in general” is that if the hazard ratio happens to be constant over time then it coincides with the difference in survival curves on the complimentary log log scale, which is an unconditional expectation.\n\n\nWhich definition is better?\nGiven that both the hazard ratio and total effect have the same property that they can be less than 1 despite no true benefit, I prefer Definition A to Definition B.\nBoth the hazard ratio and the total effect require deep thought to understand. Neither has a straightforward interpretation. To use definition B, and therefore make the distinction that the total effect is causal and the hazard ratio in not causal, is a pure technicality in my eyes."
  },
  {
    "objectID": "posts/conditional_inference/index.html",
    "href": "posts/conditional_inference/index.html",
    "title": "Covariate adjustment in RCTs: should we condition on baseline observations?",
    "section": "",
    "text": "Consider a randomized controlled trial (RCT) with patients \\(i=1,..., N\\), randomized to treatment \\(A_i = 1\\) or \\(A_i=0\\) with probability \\(1/2\\). Suppose there is a continuous outcome \\(Y_i\\).\nShould we perform inference conditional on the observed \\(A\\)? For example, are we more interested in a confidence interval \\(CI_{A}\\) for \\(\\theta\\) (a treatment effect, say) such that\n\\[P(\\theta \\in CI_A \\mid A) = 1 - \\alpha ?\\]\nOr would we prefer to not condition on \\(A\\), and aim for a confidence interval \\(CI\\) such that\n\\[P(\\theta \\in CI) = 1 - \\alpha \\] across multiple realizations of the experiment, each with different \\(A\\)’s.\nAs a concrete example, for a simple Gaussian model with known variance \\(\\sigma^2\\), suppose \\(N=500\\) but we end up with \\(N_1= 230\\) and \\(N_0= 270\\). Do we prefer to estimate the variance of the difference-in-means estimator via\n\\(\\frac{\\sigma^2}{230} + \\frac{\\sigma^2}{270}\\)\nor via\n\\(\\frac{\\sigma^2}{250} + \\frac{\\sigma^2}{250} ?\\)\nI think it’s fair to say that a majority would prefer the first option, i.e., conditional on \\(A\\)."
  },
  {
    "objectID": "posts/conditional_inference/index.html#my-thoughts-on-this-example",
    "href": "posts/conditional_inference/index.html#my-thoughts-on-this-example",
    "title": "Covariate adjustment in RCTs: should we condition on baseline observations?",
    "section": "My thoughts on this example",
    "text": "My thoughts on this example\nFor me, neither \\(CI_A\\) nor \\(CI_{A,X}\\) feel satisfactory in this example. I guess this just shows that I like to condition, since there is nothing technically wrong with \\(CI_A\\).\nMy takeaway is that we need to use a half-decent model. This may conflict with the need to pre-specify a primary analysis method in an RCT and we need to find a pragmatic way through that conflict. I don’t think \\(CI_A\\) necessarily saves us from this."
  },
  {
    "objectID": "posts/normal_inverse_wishart/index.html",
    "href": "posts/normal_inverse_wishart/index.html",
    "title": "Simulate from a Normal-inverse-Wishart distribution",
    "section": "",
    "text": "Given some data which I assume is multivariate Gaussian, I’d like to simulate from the posterior distribution of the model parameters.\nThe conjugate Bayesian analysis is a Normal-inverse-Wishart distribution.\nI’d like to do this very quickly based on snippets from wikipedia, rather than thinking much about it."
  },
  {
    "objectID": "posts/normal_inverse_wishart/index.html#snippets-from-wikipedia",
    "href": "posts/normal_inverse_wishart/index.html#snippets-from-wikipedia",
    "title": "Simulate from a Normal-inverse-Wishart distribution",
    "section": "Snippets from wikipedia",
    "text": "Snippets from wikipedia\nHere is the first snippet…\n\nTo sample from the joint posterior of \\((\\mu, \\Sigma)\\), one simply draws samples from \\(\\Sigma \\mid y \\sim W^{-1}(\\Psi_n, \\nu_n)\\), then draw \\(\\mu \\mid \\Sigma, y \\sim N_p(\\mu_n, \\Sigma / \\lambda_n)\\), where\n\n\\[\\mu_n = \\frac{\\lambda\\mu_0+n\\bar{y}}{\\lambda + n}\\]\n\\[\\lambda_n = \\lambda + n\\]\n\\[\\nu_n = \\nu + n\\]\n\\[\\Psi_n = \\Psi + S + \\frac{\\lambda n}{\\lambda + n}(\\bar{y}-\\mu_0)(\\bar{y}-\\mu_0)^T\\]\nwhere\n\\[S = \\sum_{i=1}^n  (y_i -\\bar{y})(y_i - \\bar{y})^T.\\]\nOk, so this assumes we have used a prior distribution:\n\\[\\Sigma \\sim W^{-1}(\\Psi, \\nu)\\]\n\\[\\mu \\mid \\Sigma \\sim N(\\mu_0, \\Sigma/\\lambda)\\]\nI don’t want to think too hard about prior distributions, even though that’s dangerous. It isn’t the point of this exercise. I want \\(\\mu_0\\), \\(\\lambda\\), \\(\\nu\\) and \\(\\Psi\\) to be small. Let’s just say zero for now, even if that’s a terrible idea. This way I’ve reduced the problem to drawing samples from \\(\\Sigma \\mid y \\sim W^{-1}(S, n)\\), then \\(\\mu \\mid \\Sigma, y \\sim N_p(\\bar{y}, \\Sigma / n)\\).\nThe second snippet (from a different page) is…\n\nIf \\(X\\sim W(\\Sigma, \\nu)\\), then \\(A = X^{-1}\\) has an inverse-Wishart distribution \\(A \\sim W^{-1}(\\Sigma^{-1}, \\nu).\\)\n\nOk, so this is mixing up notation now. Our target in our original notation is \\(\\Sigma \\mid y \\sim W^{-1}(S, n)\\). This snippet says we can achieve that by simulating \\(X\\) from a Wishart distribution \\(X\\sim W(S^{-1}, n)\\), and then taking \\(\\Sigma = X^{-1}\\)."
  },
  {
    "objectID": "posts/normal_inverse_wishart/index.html#example-in-r",
    "href": "posts/normal_inverse_wishart/index.html#example-in-r",
    "title": "Simulate from a Normal-inverse-Wishart distribution",
    "section": "Example in R",
    "text": "Example in R\n\n# simulate some 3-dimensional normal data, n = 20\n\nn <- 20\n\nY <- mvtnorm::rmvnorm(n = n,\n\n                      mean = c(180, 190, 200),\n\n                      sigma = matrix(70, 3, 3) + diag(30, 3))\n\n \n\n# calculate Y_bar and S\n\nY_bar <- colMeans(Y)\n\nS <- (t(Y) - Y_bar) %*% t(t(Y) - Y_bar)\n\n \n\n# simulate from inverse-Wishart distribution\n\nX <- rWishart(1, df = n, Sigma = solve(S))\n\nSigma <- solve(X[,,1])\n\nSigma\n\n          [,1]      [,2]     [,3]\n[1,] 101.98658  78.70226 77.46274\n[2,]  78.70226 104.04191 74.81375\n[3,]  77.46274  74.81375 98.78784\n\n## simulate from normal distribution\n\nmu <- mvtnorm::rmvnorm(1, mean = Y_bar, sigma = Sigma / n)\n\nmu\n\n         [,1]     [,2]     [,3]\n[1,] 180.9995 191.0862 201.4866"
  },
  {
    "objectID": "posts/old-blog-posts/index.html",
    "href": "posts/old-blog-posts/index.html",
    "title": "Old blog",
    "section": "",
    "text": "Previous blogposts can be found here"
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html",
    "href": "posts/stratify_or_adjust/index.html",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "",
    "text": "The European Medicines Agency recently released a letter of support regarding covariate adjustment in RCTs with time-to-event endpoints.\nI found it interesting, not because of the specific prognostic covariate in question, but rather the Discussion of statistical methodology section, where the more general question was addressed of whether a Cox model adjusting for any covariate is acceptable. To quote:\nThe simulation studies performed as outlined rely on the proportional hazard assumption. In case of non-proportional hazards there is the risk of a considerable increase in Type 1 error Jiang H et al.,(Stat Med (2008) when using adjusted Cox regression […] The standard approach to analysis of time-to-event data in clinical trials in oncology is to use a log-rank test, with or without stratification factors, for hypothesis testing for a primary time-to-event endpoint. Cox Models with adjustment for covariates are only used for estimation of treatment effects.\nIn informal conversations with colleagues working in oncology over recent years, I’ve often heard that performing a stratified log-rank test is considered OK, but including a (continuous, say) covariate as an additional term in a Cox model, and then performing the primary hypothesis test via this model, would be dismissed out of hand.\nI’ve wondered where this received wisdom comes from. My intuition was that, having made the assumptions necessary to perform the stratified log-rank test, it’s a relatively mild additional assumption to use the Cox model with the covariate as an additional term instead.\nSo I’m glad that this letter illuminates (perhaps) the source of this line of argument.\nIn Jiang H et al.,Stat Med (2008), the authors simulate survival times form a log-normal distribution…\n\\[\\log(T) \\sim N(-4 + 0 \\times Z_1 - 0.15 \\times Z_2  , 1).\\]\nHere, \\(Z_1\\) is the treatment indicator, and \\(Z_2\\) is a covariate with distribution \\(N(\\mu = 50, \\sigma^2 = 100)\\).\nA Cox model \\(h(t) = h_0(t)\\exp(\\beta_1 Z_1 + \\beta_2 Z_2)\\) is used for analysis with a test based on the estimated \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html#case-1-no-censoring",
    "href": "posts/stratify_or_adjust/index.html#case-1-no-censoring",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "Case 1: no censoring",
    "text": "Case 1: no censoring\nJiang H et al.,Stat Med (2008) claim that the two-sided type I error rate increases from nominal 5% to about 10%. I’ll reproduce that here (although I’ll show the one-sided alpha increases from 2.5% to about 5%)…\n\nlibrary(survival)\n \n## function to simulate data from one trial,\n## fit Cox model, and estimate adjusted hazard\n## ratio (treatment effect, beta_1)\n\nsim_one_trial <- function(n, beta_0 = -4, beta_2 = -0.15){\n\n  z_1 <- rep(c(\"c\", \"e\"), each = n)\n  z_2 <- rnorm(2 * n, mean = 50, sd = 10)\n  t_c_e <- exp(rnorm(2 * n, mean = beta_0 + beta_2 * z_2))\n  events <- rep(1, 2 * n)\n\n  fit <- coxph(Surv(t_c_e, events) ~  z_1 + z_2)\n\n  z <- summary(fit)$coef[1, \"z\"]\n}\n\n## simulate 1000 trials, with n = 200 per arm\nset.seed(325)\nres <- purrr::map_dbl(rep(200, 1000), sim_one_trial)\n \n## estimate type I error rate\nmean(res < qnorm(0.025))\n\n[1] 0.054\n\n\nSo that’s it then? This approach is a no-no?\nWell…maybe. But first let’s take a closer look at this model and this particular parameter constellation. To do that I simulate 1,000 observations from the model, and then plot the Kaplan-Meier estimate…\n\nn <- 1000\nbeta_0 = -4\nbeta_2 = -0.15\nz_2 <- rnorm(n, mean = 50, sd = 10)\nsurv_t <- exp(rnorm(n, mean = beta_0 + beta_2 * z_2))\nfit_km <- survfit(Surv(surv_t, rep(1, n)) ~ 1)\nplot(fit_km, conf.int = FALSE)\n\n\n\n\nThis is a strange choice of time scale. But that’s not really important. I can multiply the survival times by 100,000 to get a more intuitive scale (years, say). Let’s try again…\n\nsurv_t <- surv_t * 100000\nfit_km <- survfit(Surv(surv_t, rep(1, n)) ~ 1)\nplot(fit_km, conf.int = FALSE, xlim = c(0,10),\n     xlab = \"time (years)\",\n     ylab = \"P(Survival)\")\n\n\n\n\nThe next thing I’ll do is plot the KM curve of patients with an above average covariate value, \\(Z_2 > 50\\), versus those with a below average covariate value, \\(Z_2 < 50\\).\n\nbelow_50 <- survfit(Surv(surv_t[z_2 < 50], rep(1, n)[z_2 < 50]) ~ 1)\nabove_50 <- survfit(Surv(surv_t[z_2 > 50], rep(1, n)[z_2 > 50]) ~ 1)\nplot(fit_km, conf.int = FALSE, xlim = c(0,10),\n     xlab = \"time (years)\",\n     ylab = \"P(Survival)\")\n\npoints(below_50$time, below_50$surv, type = \"l\", col = 2)\npoints(above_50$time, above_50$surv, type = \"l\", col = 3)\nlegend(\"topright\", c(\"Z_2 < 50\", \"Z_2 > 50\"),\n       lty = c(1,1),\n       col = c(2,3))\n\n\n\n\nThis shows that \\(Z_2\\) is an enormously prognostic covariate. Let’s look at the period-specific average hazard ratios comparing “Z_2 > 50” versus “Z_2 < 50”, for the period [0, 1] years, and then for the period 1+ years…\n\n## year 1\ngroup <- z_2 > 50\nyear_one_t <- surv_t\nyear_one_event <- rep(1, 1000)\nyear_one_event[year_one_t > 1] <- 0\nyear_one_t[year_one_t > 1] <- 1\n\ncoxph(Surv(year_one_t, year_one_event) ~ group)\n\nCall:\ncoxph(formula = Surv(year_one_t, year_one_event) ~ group)\n\n            coef exp(coef) se(coef)     z      p\ngroupTRUE 2.1509    8.5922   0.1171 18.36 <2e-16\n\nLikelihood ratio test=456.3  on 1 df, p=< 2.2e-16\nn= 1000, number of events= 494 \n\n## beyond year 1\ngroup <- group[surv_t > 1]\nbeyond_one_t <- surv_t[surv_t > 1]\nbeyond_one_event <- rep(1, 1000)[surv_t > 1]\n\ncoxph(Surv(beyond_one_t, beyond_one_event) ~ group)\n\nCall:\ncoxph(formula = Surv(beyond_one_t, beyond_one_event) ~ group)\n\n           coef exp(coef) se(coef)     z      p\ngroupTRUE 1.352     3.867    0.121 11.17 <2e-16\n\nLikelihood ratio test=98.73  on 1 df, p=< 2.2e-16\nn= 506, number of events= 506 \n\n\nOk, so the average hazard ratio between these two groups is about 9 during the first year (where around half of the events occur), then the average hazard ratio is much lower (but still high) after year one.\nThat’s kind of extreme. Even an optimistic view of the benefits of covariate adjustment only considers HRs between 2 and 5.\nWhat happens when we change the parameter constellation so that \\(Z_2\\) is still very strongly prognostic, but perhaps something a bit more reasonable? Something like this…\n\nn <- 1000\nbeta_0 = -4 - 0.075 * 50\nbeta_2 = -0.15 + 0.075\nz_2 <- rnorm(n, mean = 50, sd = 10)\nsurv_t <- exp(rnorm(n, mean = beta_0 + beta_2 * z_2)) * 100000\n\nfit_km <- survfit(Surv(surv_t, rep(1, n)) ~ 1)\nbelow_50 <- survfit(Surv(surv_t[z_2 < 50], rep(1, n)[z_2 < 50]) ~ 1)\nabove_50 <- survfit(Surv(surv_t[z_2 > 50], rep(1, n)[z_2 > 50]) ~ 1)\n\nplot(fit_km, conf.int = FALSE, xlim = c(0,10))\npoints(below_50$time, below_50$surv, type = \"l\", col = 2)\npoints(above_50$time, above_50$surv, type = \"l\", col = 3)\nlegend(\"topright\", c(\"Z_2 < 50\", \"Z_2 > 50\"),\n       lty = c(1,1),\n       col = c(2,3))\n\n\n\n\nThen, repeating the simulation to estimate the type 1 error rate…\n\nres <- purrr::map_dbl(rep(200, 10000), sim_one_trial,\n                      beta_0 = -4 - 0.075 * 50, beta_2 = -0.15 + 0.075)\n\nmean(res < qnorm(0.025))\n\n[1] 0.0376"
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html#case-2-with-censoring",
    "href": "posts/stratify_or_adjust/index.html#case-2-with-censoring",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "Case 2: with censoring",
    "text": "Case 2: with censoring\nJiang H et al.,Stat Med (2008) show that with censoring rates of 20% and 40%, the type 1 error inflation is similar to the no censoring case.\nHowever, their censoring mechanism is a competing exponential distribution. I don’t think this is realistic. Owing to the heavy tail of the log-normal distribution, these trials would carry on for decades.\nLet’s repeat the simulations but with administrative censoring driven by a trial recruitment process that is uniform and lasts one year, plus a total trial duration of three years…\n\n## function to simulate data from one trial,\n## fit Cox model, and estimate adjusted hazard\n## ratio (treatment effect, beta_1)\nsim_one_trial_with_censoring <- function(n, beta_0 = -4, beta_2 = -0.15,\n                                         trial_duration = 3e-06, rec_duration = 1e-06){\n\n  z_1 <- rep(c(\"c\", \"e\"), each = n)\n  z_2 <- rnorm(2 * n, mean = 50, sd = 10)\n  t_c_e <- exp(rnorm(2 * n, mean = beta_0 + beta_2 * z_2))\n  rec <- runif(2 * n, 0, rec_duration)\n  events <- rec + t_c_e < trial_duration\n  t_c_e[events == 0] <- (trial_duration - rec)[events == 0]\n\n  fit <- coxph(Surv(t_c_e, events) ~  z_1 + z_2)\n  \n  z <- summary(fit)$coef[1, \"z\"]\n\n}\n\nFirstly for the original (extreme) example…\n\n## simulate 1000 trials, with n = 200 per arm\nres <- purrr::map_dbl(rep(200, 10000), sim_one_trial_with_censoring)\n\n## estimate type I error rate\nmean(res < qnorm(0.025))\n\n[1] 0.0461\n\n\nand then for the slightly more realistic one…\n\n## simulate 1000 trials, with n = 200 per arm\n\nres <- purrr::map_dbl(rep(200, 10000), sim_one_trial_with_censoring,\n                beta_0 = -4 - 0.075 * 50,\n                      beta_2 = -0.15 + 0.075)\n\n## estimate type I error rate\nmean(res < qnorm(0.025))\n\n[1] 0.0287"
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html#conclusions",
    "href": "posts/stratify_or_adjust/index.html#conclusions",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "Conclusions",
    "text": "Conclusions\nMy intuition before writing this blogpost was that I wouldn’t be too concerned about type 1 error inflation when fitting a Cox model with a continuous covariate entering as a simple linear term, even when that model is inevitably wrong.\nHave I changed my mind? Not yet. I’m guessing, with enough effort, someone could produce a realistic-looking example where there is a considerable increase in Type 1 error. But, in my opinion, the example cited by the EMA in their letter doesn’t provide that (it’s no slam dunk, at least)."
  },
  {
    "objectID": "posts/variance_lm/variance_lm.html",
    "href": "posts/variance_lm/variance_lm.html",
    "title": "Variance estimation: ANCOVA RCT analysis",
    "section": "",
    "text": "Consider a 2-arm RCT (1:1 allocation) with a continuous outcome, total sample size 200 and seven baseline covariates. Consider a superpopulation set-up. The estimand is the (super) population average treatment effect \\(\\theta = E(Y(1) - Y(0))\\). Estimation is performed via a linear regression working model\n\\[E(Y_i \\mid A_i, X_{1,i},\\ldots,X_{7,i}) = \\beta_0 + \\theta A_i + \\beta_1 X_{1,i} + \\cdots + \\beta_7{X_{7,i}}\\]\nso that \\(\\hat{\\theta}\\) is the usual least squares estimator of \\(\\theta\\). For example,\nHow should we estimate the variance of \\(\\hat{\\theta}\\)? One option is the usual ANCOVA approach,\nAlternatively, we could use an influence function approach, as for example described in Ye et al. (2023) “Toward Better Practice of Covariate Adjustment in Analyzing Randomized Clinical Trials”, which has the endorsement of being included in recent FDA guidance, and is implemented in {RobinCar2},\nThis estimated variance is 15% smaller than the usual ANCOVA variance estimate\nSame model, same data, 15% smaller estimated variance."
  },
  {
    "objectID": "posts/variance_lm/variance_lm.html#so-what",
    "href": "posts/variance_lm/variance_lm.html#so-what",
    "title": "Variance estimation: ANCOVA RCT analysis",
    "section": "So what?",
    "text": "So what?\nThe methods included in {RobinCar2} are powerful and useful. I’m an advocate for using more covariate adjustment in the primary analysis of RCTs. I’m especially excited about the methods for covariate adjustment involving time-to-event outcomes. They need to be used in the appropriate settings, however.\nIt’s easy to say that’s when \\(p\\) is not too large, and \\(n\\) is not too small. I chose this example to be somewhere on the boundary of what I would instinctively consider reasonable but can still lead to a dramatic difference between the model-based and influence function approaches.\nI’m slowly building an understanding of what’s driving this difference. It’s a combination of several factors: conditional vs unconditional inference, variance inflation factors (see Senn et al.,2024), and a degrees of freedom correction. In large(ish) sample sizes each of these factors might not seem to make a huge difference in isolation, but together it could add up to a big difference."
  },
  {
    "objectID": "posts/variance_lm_2/variance_lm_2.html",
    "href": "posts/variance_lm_2/variance_lm_2.html",
    "title": "Variance estimation: ANCOVA RCT analysis (part 2)",
    "section": "",
    "text": "In my previous post on estimating the variance of treatment effect estimators based on linear regression models in RCTs, I perhaps suggested that the method described in Ye et al. (2023) and implemented in {RobinCar} and {RobinCar2} was (for one particular scenario) underestimating the variance by about 15%. I should clarify that (in this scenario) it only underestimates the conditional variance (conditional on the particular design matrix that I chose) by about 15%. It underestimates the unconditional variance (where we consider sampling a new design matrix for each hypothetical repetition of the experiment) by about 7%, as I’ll show below.\nMy previous post contained some R code to simulate one trial and then analyze it in using a linear model with a couple of different methods of variance estimation. Here, I have simply copied that code into a function. I have added a couple of extra features: I calculate the variance with an additional two methods (sandwich estimators HC0 and HC1), and I also calculate the variance inflation factor (see Senn et al.,2024)\nNow I’ll repeat what I did in the previous post 10,000 times and store the results.\nIn this case I have used a new design matrix in each replication of the experiment. If I wanted to know the true unconditional variance of the point estimator, one thing I could do is look at the sample variance of the 10,000 point estimates,\nBut in this scenario, because I know that the true residual standard deviation is \\(1\\) (and therefore don’t need to estimate this part), a more accurate estimate of the true unconditional variance of the point estimator is\nWe can now see whether the different variance estimators are correct on average across the 10,000 experiments\nThe method from {RobinCar2} is (in this scenario) underestimating the unconditional variance by about 7%\nLooking across the different estimators, the reason HC0 underestimates the “true” variance is that it doesn’t pay a degrees of freedom penalty of \\(n / (n - 9)\\) (there are 9 coefficients in total in this linear model). If we add this penalty we get\nwhich is exactly what HC1 does:\nThe method in {RobinCar2} also doesn’t fully adjust for the degrees of freedom, but applying a correction is still not sufficient to match the “true” variance\nthat’s because it also ignores a term called the variance inflation factor (see Senn et al.,2024). In this scenario, the expected value of the variance inflation factor (calculated using the simulations) is\nIf we were to multiply the RobinCar variance by the expected value of the variance inflation factor, as well as the degrees of freedom penalty, then we would get back to approximately the correct unconditional variance"
  },
  {
    "objectID": "posts/variance_lm_2/variance_lm_2.html#conditional-inference",
    "href": "posts/variance_lm_2/variance_lm_2.html#conditional-inference",
    "title": "Variance estimation: ANCOVA RCT analysis (part 2)",
    "section": "Conditional inference",
    "text": "Conditional inference\nIn my previous post, I chose a particular design matrix by fixing the seed to be 620 before I simulated the covariates. Now let’s simulate the trial 10,000 times using that same design matrix…\n\nres_620 <- purrr::map_df(rep(200, 1e4), sim_one_trial, beta = c(0, 0.2, rep(0.1, 7)), x_seed = 620)\n\nAgain we can look at the ‘true’ conditional variance of the point estimator (where we condition on this particular design matrix)\n\nest_true_cond_var <- mean(res_620$vif) * 4 / 200\nest_true_cond_var\n\n[1] 0.02257612\n\n\nand see how well the variance estimators do on average\n\nav_res_620 <- colMeans(res_620)\nav_res_620[3:6]\n\n var_model    var_HC0    var_HC1  var_robin \n0.02252817 0.02146240 0.02247372 0.01925754 \n\n\nHere, we see that the method in RobinCar is underestimating the conditional variance by about 15% as I suggested in my previous post. The reason for the 15% (versus the 7% unconditionally) is that I chose a particularly unbalanced design matrix where the variance inflation factor is about 12%.\n\nav_res_620[\"vif\"]\n\n     vif \n1.128806 \n\n\nIf we correct for this variance inflation factor, as well as the degrees of freedom, we would arrive back to approximately the right place\n\nunname(av_res_620[\"var_robin\"] * (200 - 2) / (200 - 9) * av_res_620[\"vif\"])\n\n[1] 0.02253471"
  }
]