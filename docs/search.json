[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A place to share updates of my work on statistical methodology in clinical trials."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Covariate adjustment in RCTs: should we condition on baseline observations?\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nStratified or covariate adjusted Cox model?\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nOld blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/conditional_inference/index.html",
    "href": "posts/conditional_inference/index.html",
    "title": "Covariate adjustment in RCTs: should we condition on baseline observations?",
    "section": "",
    "text": "Consider a randomized controlled trial (RCT) with patients \\(i=1,..., N\\), randomized to treatment \\(A_i = 1\\) or \\(A_i=0\\) with probability \\(1/2\\). Suppose there is a continuous outcome \\(Y_i\\).\nShould we perform inference conditional on the observed \\(A\\)? For example, are we more interested in a confidence interval \\(CI_{A}\\) for \\(\\theta\\) (a treatment effect, say) such that\n\\[P(\\theta \\in CI_A \\mid A) = 1 - \\alpha ?\\]\nOr would we prefer to not condition on \\(A\\), and aim for a confidence interval \\(CI\\) such that\n\\[P(\\theta \\in CI) = 1 - \\alpha \\] across multiple realizations of the experiment, each with different \\(A\\)’s.\nAs a concrete example, for a simple Gaussian model with known variance \\(\\sigma^2\\), suppose \\(N=500\\) but we end up with \\(N_1= 230\\) and \\(N_0= 270\\). Do we prefer to estimate the variance of the difference-in-means estimator via\n\\(\\frac{\\sigma^2}{230} + \\frac{\\sigma^2}{270}\\)\nor via\n\\(\\frac{\\sigma^2}{250} + \\frac{\\sigma^2}{250} ?\\)\nI think it’s fair to say that a majority would prefer the first option, i.e., conditional on \\(A\\)."
  },
  {
    "objectID": "posts/conditional_inference/index.html#my-thoughts-on-this-example",
    "href": "posts/conditional_inference/index.html#my-thoughts-on-this-example",
    "title": "Covariate adjustment in RCTs: should we condition on baseline observations?",
    "section": "My thoughts on this example",
    "text": "My thoughts on this example\nFor me, neither \\(CI_A\\) nor \\(CI_{A,X}\\) feel satisfactory in this example. I guess this just shows that I like to condition, since there is nothing technically wrong with \\(CI_A\\).\nMy takeaway is that we need to use a half-decent model. This may conflict with the need to pre-specify a primary analysis method in an RCT and we need to find a pragmatic way through that conflict. I don’t think \\(CI_A\\) necessarily saves us from this."
  },
  {
    "objectID": "posts/old-blog-posts/index.html",
    "href": "posts/old-blog-posts/index.html",
    "title": "Old blog",
    "section": "",
    "text": "Previous blogposts can be found here"
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html",
    "href": "posts/stratify_or_adjust/index.html",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "",
    "text": "The European Medicines Agency recently released a letter of support regarding covariate adjustment in RCTs with time-to-event endpoints.\nI found it interesting, not because of the specific prognostic covariate in question, but rather the Discussion of statistical methodology section, where the more general question was addressed of whether a Cox model adjusting for any covariate is acceptable. To quote:\nThe simulation studies performed as outlined rely on the proportional hazard assumption. In case of non-proportional hazards there is the risk of a considerable increase in Type 1 error Jiang H et al.,(Stat Med (2008) when using adjusted Cox regression […] The standard approach to analysis of time-to-event data in clinical trials in oncology is to use a log-rank test, with or without stratification factors, for hypothesis testing for a primary time-to-event endpoint. Cox Models with adjustment for covariates are only used for estimation of treatment effects.\nIn informal conversations with colleagues working in oncology over recent years, I’ve often heard that performing a stratified log-rank test is considered OK, but including a (continuous, say) covariate as an additional term in a Cox model, and then performing the primary hypothesis test via this model, would be dismissed out of hand.\nI’ve wondered where this received wisdom comes from. My intuition was that, having made the assumptions necessary to perform the stratified log-rank test, it’s a relatively mild additional assumption to use the Cox model with the covariate as an additional term instead.\nSo I’m glad that this letter illuminates (perhaps) the source of this line of argument.\nIn Jiang H et al.,Stat Med (2008), the authors simulate survival times form a log-normal distribution…\n\\[\\log(T) \\sim N(-4 + 0 \\times Z_1 - 0.15 \\times Z_2  , 1).\\]\nHere, \\(Z_1\\) is the treatment indicator, and \\(Z_2\\) is a covariate with distribution \\(N(\\mu = 50, \\sigma^2 = 100)\\).\nA Cox model \\(h(t) = h_0(t)\\exp(\\beta_1 Z_1 + \\beta_2 Z_2)\\) is used for analysis with a test based on the estimated \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html#case-1-no-censoring",
    "href": "posts/stratify_or_adjust/index.html#case-1-no-censoring",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "Case 1: no censoring",
    "text": "Case 1: no censoring\nJiang H et al.,Stat Med (2008) claim that the two-sided type I error rate increases from nominal 5% to about 10%. I’ll reproduce that here (although I’ll show the one-sided alpha increases from 2.5% to about 5%)…\n\nlibrary(survival)\n \n## function to simulate data from one trial,\n## fit Cox model, and estimate adjusted hazard\n## ratio (treatment effect, beta_1)\n\nsim_one_trial <- function(n, beta_0 = -4, beta_2 = -0.15){\n\n  z_1 <- rep(c(\"c\", \"e\"), each = n)\n  z_2 <- rnorm(2 * n, mean = 50, sd = 10)\n  t_c_e <- exp(rnorm(2 * n, mean = beta_0 + beta_2 * z_2))\n  events <- rep(1, 2 * n)\n\n  fit <- coxph(Surv(t_c_e, events) ~  z_1 + z_2)\n\n  z <- summary(fit)$coef[1, \"z\"]\n}\n\n## simulate 1000 trials, with n = 200 per arm\nset.seed(325)\nres <- purrr::map_dbl(rep(200, 1000), sim_one_trial)\n \n## estimate type I error rate\nmean(res < qnorm(0.025))\n\n[1] 0.054\n\n\nSo that’s it then? This approach is a no-no?\nWell…maybe. But first let’s take a closer look at this model and this particular parameter constellation. To do that I simulate 1,000 observations from the model, and then plot the Kaplan-Meier estimate…\n\nn <- 1000\nbeta_0 = -4\nbeta_2 = -0.15\nz_2 <- rnorm(n, mean = 50, sd = 10)\nsurv_t <- exp(rnorm(n, mean = beta_0 + beta_2 * z_2))\nfit_km <- survfit(Surv(surv_t, rep(1, n)) ~ 1)\nplot(fit_km, conf.int = FALSE)\n\n\n\n\nThis is a strange choice of time scale. But that’s not really important. I can multiply the survival times by 100,000 to get a more intuitive scale (years, say). Let’s try again…\n\nsurv_t <- surv_t * 100000\nfit_km <- survfit(Surv(surv_t, rep(1, n)) ~ 1)\nplot(fit_km, conf.int = FALSE, xlim = c(0,10),\n     xlab = \"time (years)\",\n     ylab = \"P(Survival)\")\n\n\n\n\nThe next thing I’ll do is plot the KM curve of patients with an above average covariate value, \\(Z_2 > 50\\), versus those with a below average covariate value, \\(Z_2 < 50\\).\n\nbelow_50 <- survfit(Surv(surv_t[z_2 < 50], rep(1, n)[z_2 < 50]) ~ 1)\nabove_50 <- survfit(Surv(surv_t[z_2 > 50], rep(1, n)[z_2 > 50]) ~ 1)\nplot(fit_km, conf.int = FALSE, xlim = c(0,10),\n     xlab = \"time (years)\",\n     ylab = \"P(Survival)\")\n\npoints(below_50$time, below_50$surv, type = \"l\", col = 2)\npoints(above_50$time, above_50$surv, type = \"l\", col = 3)\nlegend(\"topright\", c(\"Z_2 < 50\", \"Z_2 > 50\"),\n       lty = c(1,1),\n       col = c(2,3))\n\n\n\n\nThis shows that \\(Z_2\\) is an enormously prognostic covariate. Let’s look at the period-specific average hazard ratios comparing “Z_2 > 50” versus “Z_2 < 50”, for the period [0, 1] years, and then for the period 1+ years…\n\n## year 1\ngroup <- z_2 > 50\nyear_one_t <- surv_t\nyear_one_event <- rep(1, 1000)\nyear_one_event[year_one_t > 1] <- 0\nyear_one_t[year_one_t > 1] <- 1\n\ncoxph(Surv(year_one_t, year_one_event) ~ group)\n\nCall:\ncoxph(formula = Surv(year_one_t, year_one_event) ~ group)\n\n            coef exp(coef) se(coef)     z      p\ngroupTRUE 2.1509    8.5922   0.1171 18.36 <2e-16\n\nLikelihood ratio test=456.3  on 1 df, p=< 2.2e-16\nn= 1000, number of events= 494 \n\n## beyond year 1\ngroup <- group[surv_t > 1]\nbeyond_one_t <- surv_t[surv_t > 1]\nbeyond_one_event <- rep(1, 1000)[surv_t > 1]\n\ncoxph(Surv(beyond_one_t, beyond_one_event) ~ group)\n\nCall:\ncoxph(formula = Surv(beyond_one_t, beyond_one_event) ~ group)\n\n           coef exp(coef) se(coef)     z      p\ngroupTRUE 1.352     3.867    0.121 11.17 <2e-16\n\nLikelihood ratio test=98.73  on 1 df, p=< 2.2e-16\nn= 506, number of events= 506 \n\n\nOk, so the average hazard ratio between these two groups is about 9 during the first year (where around half of the events occur), then the average hazard ratio is much lower (but still high) after year one.\nThat’s kind of extreme. Even an optimistic view of the benefits of covariate adjustment only considers HRs between 2 and 5.\nWhat happens when we change the parameter constellation so that \\(Z_2\\) is still very strongly prognostic, but perhaps something a bit more reasonable? Something like this…\n\nn <- 1000\nbeta_0 = -4 - 0.075 * 50\nbeta_2 = -0.15 + 0.075\nz_2 <- rnorm(n, mean = 50, sd = 10)\nsurv_t <- exp(rnorm(n, mean = beta_0 + beta_2 * z_2)) * 100000\n\nfit_km <- survfit(Surv(surv_t, rep(1, n)) ~ 1)\nbelow_50 <- survfit(Surv(surv_t[z_2 < 50], rep(1, n)[z_2 < 50]) ~ 1)\nabove_50 <- survfit(Surv(surv_t[z_2 > 50], rep(1, n)[z_2 > 50]) ~ 1)\n\nplot(fit_km, conf.int = FALSE, xlim = c(0,10))\npoints(below_50$time, below_50$surv, type = \"l\", col = 2)\npoints(above_50$time, above_50$surv, type = \"l\", col = 3)\nlegend(\"topright\", c(\"Z_2 < 50\", \"Z_2 > 50\"),\n       lty = c(1,1),\n       col = c(2,3))\n\n\n\n\nThen, repeating the simulation to estimate the type 1 error rate…\n\nres <- purrr::map_dbl(rep(200, 10000), sim_one_trial,\n                      beta_0 = -4 - 0.075 * 50, beta_2 = -0.15 + 0.075)\n\nmean(res < qnorm(0.025))\n\n[1] 0.0376"
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html#case-2-with-censoring",
    "href": "posts/stratify_or_adjust/index.html#case-2-with-censoring",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "Case 2: with censoring",
    "text": "Case 2: with censoring\nJiang H et al.,Stat Med (2008) show that with censoring rates of 20% and 40%, the type 1 error inflation is similar to the no censoring case.\nHowever, their censoring mechanism is a competing exponential distribution. I don’t think this is realistic. Owing to the heavy tail of the log-normal distribution, these trials would carry on for decades.\nLet’s repeat the simulations but with administrative censoring driven by a trial recruitment process that is uniform and lasts one year, plus a total trial duration of three years…\n\n## function to simulate data from one trial,\n## fit Cox model, and estimate adjusted hazard\n## ratio (treatment effect, beta_1)\nsim_one_trial_with_censoring <- function(n, beta_0 = -4, beta_2 = -0.15,\n                                         trial_duration = 3e-06, rec_duration = 1e-06){\n\n  z_1 <- rep(c(\"c\", \"e\"), each = n)\n  z_2 <- rnorm(2 * n, mean = 50, sd = 10)\n  t_c_e <- exp(rnorm(2 * n, mean = beta_0 + beta_2 * z_2))\n  rec <- runif(2 * n, 0, rec_duration)\n  events <- rec + t_c_e < trial_duration\n  t_c_e[events == 0] <- (trial_duration - rec)[events == 0]\n\n  fit <- coxph(Surv(t_c_e, events) ~  z_1 + z_2)\n  \n  z <- summary(fit)$coef[1, \"z\"]\n\n}\n\nFirstly for the original (extreme) example…\n\n## simulate 1000 trials, with n = 200 per arm\nres <- purrr::map_dbl(rep(200, 10000), sim_one_trial_with_censoring)\n\n## estimate type I error rate\nmean(res < qnorm(0.025))\n\n[1] 0.0461\n\n\nand then for the slightly more realistic one…\n\n## simulate 1000 trials, with n = 200 per arm\n\nres <- purrr::map_dbl(rep(200, 10000), sim_one_trial_with_censoring,\n                beta_0 = -4 - 0.075 * 50,\n                      beta_2 = -0.15 + 0.075)\n\n## estimate type I error rate\nmean(res < qnorm(0.025))\n\n[1] 0.0287"
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html#conclusions",
    "href": "posts/stratify_or_adjust/index.html#conclusions",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "Conclusions",
    "text": "Conclusions\nMy intuition before writing this blogpost was that I wouldn’t be too concerned about type 1 error inflation when fitting a Cox model with a continuous covariate entering as a simple linear term, even when that model is inevitably wrong.\nHave I changed my mind? Not yet. I’m guessing, with enough effort, someone could produce a realistic-looking example where there is a considerable increase in Type 1 error. But, in my opinion, the example cited by the EMA in their letter doesn’t provide that (it’s no slam dunk, at least)."
  }
]