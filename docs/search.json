[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A place to share updates of my work on statistical methodology in clinical trials."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Causal interpretation of the hazard ratio in RCTs\n\n\n\n\n\n\n\nCausal inference\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nSimulate from a Normal-inverse-Wishart distribution\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nCovariate adjustment in RCTs: should we condition on baseline observations?\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nStratified or covariate adjusted Cox model?\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\n\n\nOld blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nDominic Magirr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/causal_hr/causal_hr.html",
    "href": "posts/causal_hr/causal_hr.html",
    "title": "Causal interpretation of the hazard ratio in RCTs",
    "section": "",
    "text": "Motivation\nA new set of articles on the “Causal interpretation of the hazard ratio in randomized clinical trials” ( original article, commentary, response) has pushed me to write this blogpost. It’s something I’ve been considering for a while, not because it’s the most important thing in the world, but there seems to be a persistent confusion, despite some great attempts to clarify\n\n\nSource of the confusion\nThe mistake (in my eyes) is to claim something like “the hazard ratio is not a causal effect” (in the context of an RCT) without first providing a precise definition of “causal effect”.\n\n\nDefinition A\nOne clean and precise definition of a causal effect is provided by Hernan & Robins Technical Point 1.1…\nIn general, a population causal effect can be defined as a contrast of any functional of the marginal distributions of counterfactual outcomes under different actions or treatment values.\nThe hazard ratio, \\(hr(t)\\), satisfies this definition, for any choice of \\(t\\). Therefore, if we follow this definition, then the hazard ratio is a causal effect. End of story. We can all move on.\n\n\nCriticism of hazard ratios in loose language\n“The hazards of hazard ratios” (Hernan, 2010) contains an example where the hazard ratio (hormone therapy vs control) after 5 years is less than 1 “even if hormone therapy has no truly preventive effect in any woman at any time.”\nInterestingly, though, the fact that a hazard ratio can ostensibly point in favour of an experimental treatment despite no true benefit is not the grounds given for claiming that a hazard ratio lacks a causal interpretation. For example, the so-called “total effect” in a competing risks setting also has the property that it can be less than 1 (if it’s a ratio) despite no true benefit. Yet this is unequivocally a causal effect according to the authors.\nRather, the grounds given for claiming the hazard ratio is not a causal effect is always given in loose language, e.g., it somehow does not compare equal groups, or has a so-called “in-built selection bias”. This is ambiguous. I would say that a hazard ratio at 5 years is a consequence of everything that happens to everyone in the first 5 years. It is not merely a comparison of the set of patients alive at 5 years, as if the patients who have died had never existed.\n\n\nDefinition B\nI’ve not seen a precise definition of a causal effect that would rule out the hazard ratio, so I’ll make one up here…\nIn general, a population causal effect can be defined as a contrast of any functional (except for a conditional expectation where the conditioning is on a post-baseline event) of the marginal distributions of counterfactual outcomes under different actions or treatment values.\nIf we follow this definition, then the hazard ratio is not (in general) a causal effect. End of story. We can all move on.\nThe reason for “in general” is that if the hazard ratio happens to be constant over time then it coincides with the difference in survival curves on the complimentary log log scale, which is an unconditional expectation.\n\n\nWhich definition is better?\nGiven that both the hazard ratio and total effect have the same property that they can be less than 1 despite no true benefit, I prefer Definition A to Definition B.\nBoth the hazard ratio and the total effect require deep thought to understand. Neither has a straightforward interpretation. To use definition B, and therefore make the distinction that the total effect is causal and the hazard ratio in not causal, is a pure technicality in my eyes."
  },
  {
    "objectID": "posts/conditional_inference/index.html",
    "href": "posts/conditional_inference/index.html",
    "title": "Covariate adjustment in RCTs: should we condition on baseline observations?",
    "section": "",
    "text": "Consider a randomized controlled trial (RCT) with patients \\(i=1,..., N\\), randomized to treatment \\(A_i = 1\\) or \\(A_i=0\\) with probability \\(1/2\\). Suppose there is a continuous outcome \\(Y_i\\).\nShould we perform inference conditional on the observed \\(A\\)? For example, are we more interested in a confidence interval \\(CI_{A}\\) for \\(\\theta\\) (a treatment effect, say) such that\n\\[P(\\theta \\in CI_A \\mid A) = 1 - \\alpha ?\\]\nOr would we prefer to not condition on \\(A\\), and aim for a confidence interval \\(CI\\) such that\n\\[P(\\theta \\in CI) = 1 - \\alpha \\] across multiple realizations of the experiment, each with different \\(A\\)’s.\nAs a concrete example, for a simple Gaussian model with known variance \\(\\sigma^2\\), suppose \\(N=500\\) but we end up with \\(N_1= 230\\) and \\(N_0= 270\\). Do we prefer to estimate the variance of the difference-in-means estimator via\n\\(\\frac{\\sigma^2}{230} + \\frac{\\sigma^2}{270}\\)\nor via\n\\(\\frac{\\sigma^2}{250} + \\frac{\\sigma^2}{250} ?\\)\nI think it’s fair to say that a majority would prefer the first option, i.e., conditional on \\(A\\)."
  },
  {
    "objectID": "posts/conditional_inference/index.html#my-thoughts-on-this-example",
    "href": "posts/conditional_inference/index.html#my-thoughts-on-this-example",
    "title": "Covariate adjustment in RCTs: should we condition on baseline observations?",
    "section": "My thoughts on this example",
    "text": "My thoughts on this example\nFor me, neither \\(CI_A\\) nor \\(CI_{A,X}\\) feel satisfactory in this example. I guess this just shows that I like to condition, since there is nothing technically wrong with \\(CI_A\\).\nMy takeaway is that we need to use a half-decent model. This may conflict with the need to pre-specify a primary analysis method in an RCT and we need to find a pragmatic way through that conflict. I don’t think \\(CI_A\\) necessarily saves us from this."
  },
  {
    "objectID": "posts/normal_inverse_wishart/index.html",
    "href": "posts/normal_inverse_wishart/index.html",
    "title": "Simulate from a Normal-inverse-Wishart distribution",
    "section": "",
    "text": "Given some data which I assume is multivariate Gaussian, I’d like to simulate from the posterior distribution of the model parameters.\nThe conjugate Bayesian analysis is a Normal-inverse-Wishart distribution.\nI’d like to do this very quickly based on snippets from wikipedia, rather than thinking much about it."
  },
  {
    "objectID": "posts/normal_inverse_wishart/index.html#snippets-from-wikipedia",
    "href": "posts/normal_inverse_wishart/index.html#snippets-from-wikipedia",
    "title": "Simulate from a Normal-inverse-Wishart distribution",
    "section": "Snippets from wikipedia",
    "text": "Snippets from wikipedia\nHere is the first snippet…\n\nTo sample from the joint posterior of \\((\\mu, \\Sigma)\\), one simply draws samples from \\(\\Sigma \\mid y \\sim W^{-1}(\\Psi_n, \\nu_n)\\), then draw \\(\\mu \\mid \\Sigma, y \\sim N_p(\\mu_n, \\Sigma / \\lambda_n)\\), where\n\n\\[\\mu_n = \\frac{\\lambda\\mu_0+n\\bar{y}}{\\lambda + n}\\]\n\\[\\lambda_n = \\lambda + n\\]\n\\[\\nu_n = \\nu + n\\]\n\\[\\Psi_n = \\Psi + S + \\frac{\\lambda n}{\\lambda + n}(\\bar{y}-\\mu_0)(\\bar{y}-\\mu_0)^T\\]\nwhere\n\\[S = \\sum_{i=1}^n  (y_i -\\bar{y})(y_i - \\bar{y})^T.\\]\nOk, so this assumes we have used a prior distribution:\n\\[\\Sigma \\sim W^{-1}(\\Psi, \\nu)\\]\n\\[\\mu \\mid \\Sigma \\sim N(\\mu_0, \\Sigma/\\lambda)\\]\nI don’t want to think too hard about prior distributions, even though that’s dangerous. It isn’t the point of this exercise. I want \\(\\mu_0\\), \\(\\lambda\\), \\(\\nu\\) and \\(\\Psi\\) to be small. Let’s just say zero for now, even if that’s a terrible idea. This way I’ve reduced the problem to drawing samples from \\(\\Sigma \\mid y \\sim W^{-1}(S, n)\\), then \\(\\mu \\mid \\Sigma, y \\sim N_p(\\bar{y}, \\Sigma / n)\\).\nThe second snippet (from a different page) is…\n\nIf \\(X\\sim W(\\Sigma, \\nu)\\), then \\(A = X^{-1}\\) has an inverse-Wishart distribution \\(A \\sim W^{-1}(\\Sigma^{-1}, \\nu).\\)\n\nOk, so this is mixing up notation now. Our target in our original notation is \\(\\Sigma \\mid y \\sim W^{-1}(S, n)\\). This snippet says we can achieve that by simulating \\(X\\) from a Wishart distribution \\(X\\sim W(S^{-1}, n)\\), and then taking \\(\\Sigma = X^{-1}\\)."
  },
  {
    "objectID": "posts/normal_inverse_wishart/index.html#example-in-r",
    "href": "posts/normal_inverse_wishart/index.html#example-in-r",
    "title": "Simulate from a Normal-inverse-Wishart distribution",
    "section": "Example in R",
    "text": "Example in R\n\n# simulate some 3-dimensional normal data, n = 20\n\nn <- 20\n\nY <- mvtnorm::rmvnorm(n = n,\n\n                      mean = c(180, 190, 200),\n\n                      sigma = matrix(70, 3, 3) + diag(30, 3))\n\n \n\n# calculate Y_bar and S\n\nY_bar <- colMeans(Y)\n\nS <- (t(Y) - Y_bar) %*% t(t(Y) - Y_bar)\n\n \n\n# simulate from inverse-Wishart distribution\n\nX <- rWishart(1, df = n, Sigma = solve(S))\n\nSigma <- solve(X[,,1])\n\nSigma\n\n          [,1]      [,2]     [,3]\n[1,] 101.98658  78.70226 77.46274\n[2,]  78.70226 104.04191 74.81375\n[3,]  77.46274  74.81375 98.78784\n\n## simulate from normal distribution\n\nmu <- mvtnorm::rmvnorm(1, mean = Y_bar, sigma = Sigma / n)\n\nmu\n\n         [,1]     [,2]     [,3]\n[1,] 180.9995 191.0862 201.4866"
  },
  {
    "objectID": "posts/old-blog-posts/index.html",
    "href": "posts/old-blog-posts/index.html",
    "title": "Old blog",
    "section": "",
    "text": "Previous blogposts can be found here"
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html",
    "href": "posts/stratify_or_adjust/index.html",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "",
    "text": "The European Medicines Agency recently released a letter of support regarding covariate adjustment in RCTs with time-to-event endpoints.\nI found it interesting, not because of the specific prognostic covariate in question, but rather the Discussion of statistical methodology section, where the more general question was addressed of whether a Cox model adjusting for any covariate is acceptable. To quote:\nThe simulation studies performed as outlined rely on the proportional hazard assumption. In case of non-proportional hazards there is the risk of a considerable increase in Type 1 error Jiang H et al.,(Stat Med (2008) when using adjusted Cox regression […] The standard approach to analysis of time-to-event data in clinical trials in oncology is to use a log-rank test, with or without stratification factors, for hypothesis testing for a primary time-to-event endpoint. Cox Models with adjustment for covariates are only used for estimation of treatment effects.\nIn informal conversations with colleagues working in oncology over recent years, I’ve often heard that performing a stratified log-rank test is considered OK, but including a (continuous, say) covariate as an additional term in a Cox model, and then performing the primary hypothesis test via this model, would be dismissed out of hand.\nI’ve wondered where this received wisdom comes from. My intuition was that, having made the assumptions necessary to perform the stratified log-rank test, it’s a relatively mild additional assumption to use the Cox model with the covariate as an additional term instead.\nSo I’m glad that this letter illuminates (perhaps) the source of this line of argument.\nIn Jiang H et al.,Stat Med (2008), the authors simulate survival times form a log-normal distribution…\n\\[\\log(T) \\sim N(-4 + 0 \\times Z_1 - 0.15 \\times Z_2  , 1).\\]\nHere, \\(Z_1\\) is the treatment indicator, and \\(Z_2\\) is a covariate with distribution \\(N(\\mu = 50, \\sigma^2 = 100)\\).\nA Cox model \\(h(t) = h_0(t)\\exp(\\beta_1 Z_1 + \\beta_2 Z_2)\\) is used for analysis with a test based on the estimated \\(\\hat{\\beta}_1\\)."
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html#case-1-no-censoring",
    "href": "posts/stratify_or_adjust/index.html#case-1-no-censoring",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "Case 1: no censoring",
    "text": "Case 1: no censoring\nJiang H et al.,Stat Med (2008) claim that the two-sided type I error rate increases from nominal 5% to about 10%. I’ll reproduce that here (although I’ll show the one-sided alpha increases from 2.5% to about 5%)…\n\nlibrary(survival)\n \n## function to simulate data from one trial,\n## fit Cox model, and estimate adjusted hazard\n## ratio (treatment effect, beta_1)\n\nsim_one_trial <- function(n, beta_0 = -4, beta_2 = -0.15){\n\n  z_1 <- rep(c(\"c\", \"e\"), each = n)\n  z_2 <- rnorm(2 * n, mean = 50, sd = 10)\n  t_c_e <- exp(rnorm(2 * n, mean = beta_0 + beta_2 * z_2))\n  events <- rep(1, 2 * n)\n\n  fit <- coxph(Surv(t_c_e, events) ~  z_1 + z_2)\n\n  z <- summary(fit)$coef[1, \"z\"]\n}\n\n## simulate 1000 trials, with n = 200 per arm\nset.seed(325)\nres <- purrr::map_dbl(rep(200, 1000), sim_one_trial)\n \n## estimate type I error rate\nmean(res < qnorm(0.025))\n\n[1] 0.054\n\n\nSo that’s it then? This approach is a no-no?\nWell…maybe. But first let’s take a closer look at this model and this particular parameter constellation. To do that I simulate 1,000 observations from the model, and then plot the Kaplan-Meier estimate…\n\nn <- 1000\nbeta_0 = -4\nbeta_2 = -0.15\nz_2 <- rnorm(n, mean = 50, sd = 10)\nsurv_t <- exp(rnorm(n, mean = beta_0 + beta_2 * z_2))\nfit_km <- survfit(Surv(surv_t, rep(1, n)) ~ 1)\nplot(fit_km, conf.int = FALSE)\n\n\n\n\nThis is a strange choice of time scale. But that’s not really important. I can multiply the survival times by 100,000 to get a more intuitive scale (years, say). Let’s try again…\n\nsurv_t <- surv_t * 100000\nfit_km <- survfit(Surv(surv_t, rep(1, n)) ~ 1)\nplot(fit_km, conf.int = FALSE, xlim = c(0,10),\n     xlab = \"time (years)\",\n     ylab = \"P(Survival)\")\n\n\n\n\nThe next thing I’ll do is plot the KM curve of patients with an above average covariate value, \\(Z_2 > 50\\), versus those with a below average covariate value, \\(Z_2 < 50\\).\n\nbelow_50 <- survfit(Surv(surv_t[z_2 < 50], rep(1, n)[z_2 < 50]) ~ 1)\nabove_50 <- survfit(Surv(surv_t[z_2 > 50], rep(1, n)[z_2 > 50]) ~ 1)\nplot(fit_km, conf.int = FALSE, xlim = c(0,10),\n     xlab = \"time (years)\",\n     ylab = \"P(Survival)\")\n\npoints(below_50$time, below_50$surv, type = \"l\", col = 2)\npoints(above_50$time, above_50$surv, type = \"l\", col = 3)\nlegend(\"topright\", c(\"Z_2 < 50\", \"Z_2 > 50\"),\n       lty = c(1,1),\n       col = c(2,3))\n\n\n\n\nThis shows that \\(Z_2\\) is an enormously prognostic covariate. Let’s look at the period-specific average hazard ratios comparing “Z_2 > 50” versus “Z_2 < 50”, for the period [0, 1] years, and then for the period 1+ years…\n\n## year 1\ngroup <- z_2 > 50\nyear_one_t <- surv_t\nyear_one_event <- rep(1, 1000)\nyear_one_event[year_one_t > 1] <- 0\nyear_one_t[year_one_t > 1] <- 1\n\ncoxph(Surv(year_one_t, year_one_event) ~ group)\n\nCall:\ncoxph(formula = Surv(year_one_t, year_one_event) ~ group)\n\n            coef exp(coef) se(coef)     z      p\ngroupTRUE 2.1509    8.5922   0.1171 18.36 <2e-16\n\nLikelihood ratio test=456.3  on 1 df, p=< 2.2e-16\nn= 1000, number of events= 494 \n\n## beyond year 1\ngroup <- group[surv_t > 1]\nbeyond_one_t <- surv_t[surv_t > 1]\nbeyond_one_event <- rep(1, 1000)[surv_t > 1]\n\ncoxph(Surv(beyond_one_t, beyond_one_event) ~ group)\n\nCall:\ncoxph(formula = Surv(beyond_one_t, beyond_one_event) ~ group)\n\n           coef exp(coef) se(coef)     z      p\ngroupTRUE 1.352     3.867    0.121 11.17 <2e-16\n\nLikelihood ratio test=98.73  on 1 df, p=< 2.2e-16\nn= 506, number of events= 506 \n\n\nOk, so the average hazard ratio between these two groups is about 9 during the first year (where around half of the events occur), then the average hazard ratio is much lower (but still high) after year one.\nThat’s kind of extreme. Even an optimistic view of the benefits of covariate adjustment only considers HRs between 2 and 5.\nWhat happens when we change the parameter constellation so that \\(Z_2\\) is still very strongly prognostic, but perhaps something a bit more reasonable? Something like this…\n\nn <- 1000\nbeta_0 = -4 - 0.075 * 50\nbeta_2 = -0.15 + 0.075\nz_2 <- rnorm(n, mean = 50, sd = 10)\nsurv_t <- exp(rnorm(n, mean = beta_0 + beta_2 * z_2)) * 100000\n\nfit_km <- survfit(Surv(surv_t, rep(1, n)) ~ 1)\nbelow_50 <- survfit(Surv(surv_t[z_2 < 50], rep(1, n)[z_2 < 50]) ~ 1)\nabove_50 <- survfit(Surv(surv_t[z_2 > 50], rep(1, n)[z_2 > 50]) ~ 1)\n\nplot(fit_km, conf.int = FALSE, xlim = c(0,10))\npoints(below_50$time, below_50$surv, type = \"l\", col = 2)\npoints(above_50$time, above_50$surv, type = \"l\", col = 3)\nlegend(\"topright\", c(\"Z_2 < 50\", \"Z_2 > 50\"),\n       lty = c(1,1),\n       col = c(2,3))\n\n\n\n\nThen, repeating the simulation to estimate the type 1 error rate…\n\nres <- purrr::map_dbl(rep(200, 10000), sim_one_trial,\n                      beta_0 = -4 - 0.075 * 50, beta_2 = -0.15 + 0.075)\n\nmean(res < qnorm(0.025))\n\n[1] 0.0376"
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html#case-2-with-censoring",
    "href": "posts/stratify_or_adjust/index.html#case-2-with-censoring",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "Case 2: with censoring",
    "text": "Case 2: with censoring\nJiang H et al.,Stat Med (2008) show that with censoring rates of 20% and 40%, the type 1 error inflation is similar to the no censoring case.\nHowever, their censoring mechanism is a competing exponential distribution. I don’t think this is realistic. Owing to the heavy tail of the log-normal distribution, these trials would carry on for decades.\nLet’s repeat the simulations but with administrative censoring driven by a trial recruitment process that is uniform and lasts one year, plus a total trial duration of three years…\n\n## function to simulate data from one trial,\n## fit Cox model, and estimate adjusted hazard\n## ratio (treatment effect, beta_1)\nsim_one_trial_with_censoring <- function(n, beta_0 = -4, beta_2 = -0.15,\n                                         trial_duration = 3e-06, rec_duration = 1e-06){\n\n  z_1 <- rep(c(\"c\", \"e\"), each = n)\n  z_2 <- rnorm(2 * n, mean = 50, sd = 10)\n  t_c_e <- exp(rnorm(2 * n, mean = beta_0 + beta_2 * z_2))\n  rec <- runif(2 * n, 0, rec_duration)\n  events <- rec + t_c_e < trial_duration\n  t_c_e[events == 0] <- (trial_duration - rec)[events == 0]\n\n  fit <- coxph(Surv(t_c_e, events) ~  z_1 + z_2)\n  \n  z <- summary(fit)$coef[1, \"z\"]\n\n}\n\nFirstly for the original (extreme) example…\n\n## simulate 1000 trials, with n = 200 per arm\nres <- purrr::map_dbl(rep(200, 10000), sim_one_trial_with_censoring)\n\n## estimate type I error rate\nmean(res < qnorm(0.025))\n\n[1] 0.0461\n\n\nand then for the slightly more realistic one…\n\n## simulate 1000 trials, with n = 200 per arm\n\nres <- purrr::map_dbl(rep(200, 10000), sim_one_trial_with_censoring,\n                beta_0 = -4 - 0.075 * 50,\n                      beta_2 = -0.15 + 0.075)\n\n## estimate type I error rate\nmean(res < qnorm(0.025))\n\n[1] 0.0287"
  },
  {
    "objectID": "posts/stratify_or_adjust/index.html#conclusions",
    "href": "posts/stratify_or_adjust/index.html#conclusions",
    "title": "Stratified or covariate adjusted Cox model?",
    "section": "Conclusions",
    "text": "Conclusions\nMy intuition before writing this blogpost was that I wouldn’t be too concerned about type 1 error inflation when fitting a Cox model with a continuous covariate entering as a simple linear term, even when that model is inevitably wrong.\nHave I changed my mind? Not yet. I’m guessing, with enough effort, someone could produce a realistic-looking example where there is a considerable increase in Type 1 error. But, in my opinion, the example cited by the EMA in their letter doesn’t provide that (it’s no slam dunk, at least)."
  }
]