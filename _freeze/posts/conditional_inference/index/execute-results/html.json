{
  "hash": "babc8009db99939aa98a616f696b344e",
  "result": {
    "markdown": "---\ntitle: \"Covariate adjustment in RCTs: should we condition on baseline observations?\"\nauthor: \"Dominic Magirr\"\ndate: \"2023-07-15\"\ncategories: [news]\n---\n\n\n\n\n# To condition or not? (Part 1) \n\nConsider a randomized controlled trial (RCT) with patients $i=1,..., N$, randomized to treatment $A_i = 1$ or $A_i=0$ with probability $1/2$. Suppose there is a continuous outcome $Y_i$.\n\nShould we perform inference conditional on the observed $A$? For example, are we more interested in a confidence interval $CI_{A}$ for $\\theta$ (a treatment effect, say) such that\n\n$$P(\\theta \\in CI_A \\mid A) = 1 - \\alpha ?$$ \n\nOr would we prefer to not condition on $A$, and aim for a confidence interval $CI$ such that\n\n$$P(\\theta \\in CI) = 1 - \\alpha $$ \nacross multiple realizations of the experiment, each with different $A$'s.\n\nAs a concrete example, for a simple Gaussian model with known variance $\\sigma^2$, suppose $N=500$ but we end up with $N_1= 230$ and $N_0= 270$. Do we prefer to estimate the variance of the difference-in-means estimator via\n\n$\\frac{\\sigma^2}{230} + \\frac{\\sigma^2}{270}$\n\nor via \n\n$\\frac{\\sigma^2}{250} + \\frac{\\sigma^2}{250} ?$\n\nI think it's fair to say that a majority would prefer the first option, i.e., conditional on $A$.\n\n\n# To condition or not? (Part 2)\n\nNow suppose there is a continuous, prognostic, baseline covariate $X_i$. Should we perform inference conditional on the observed $(A,X)$? For example, should we seek a confidence interval $CI_{A,X}$ such that \n\n$$P(\\theta \\in CI_{A,X} \\mid A, X) = 1 - \\alpha $$ \nrather than a confidence interval $CI_A$ such that\n\n$$P(\\theta \\in CI_{A} \\mid A) = 1 - \\alpha ?$$ \n\nUndoubtedly, this is a harder question. There are at least two variants of it, depending on whether or not $X$ is used in the construction of $CI_A$. \n\nThe first variant, where $X$ is completely ignored in the construction of $CI_A$, has, I think, been comprehensively dealt with, for example by Stephen Senn in his [Seven Myths about Randomization](https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5713?casa_token=PP3JuqTFJd8AAAAA:kmx4NizbwzLw84KmVsuJKTGRbJKACKQ9gLnuzA62H8x8ZKUt8st30XTJVRPGPpCkpPyTRqygnaeI8g). Having read that article, no one could possibly argue it's ok to completely ignore $X$ in this situation. \n\n*Of course, if there are multiple prognostic baseline covariates, then it may not be practical to condition on all of them. But this is not the direction I want to go in this blog post. I'd rather stick with the single covariate and try to address the second variant of the question.*\n\nThe second variant of the question, where $X$ is used in the construction of $CI_A$ but we still seek inference conditional on $A$ but not $X$, is far less important than the first. In fact, the intervals $CI_A$ and $CI_{A,X}$ may even be identical, with only the interpretation different. \n\nNevertheless, it's still a question that bugs me. And it does have some, albeit minor, practical consequences. \n\n# The argument for preferring $CI_A$ over $CI_{A,X}$\n\nSuppose I fit an ANCOVA model $Y_i \\sim N(\\beta_0 + \\beta_1 A_i + \\beta_2 X_i, \\sigma^2).$ In this case, suppose both $CI_A$ and $CI_{A,X}$ are just the standard confidence interval for $\\beta_1$.\n\nWhen I try to make a statement\n\n$$P(\\beta_1 \\in CI_{A,X} \\mid A, X) = 1 - \\alpha $$\n\nI am relying on the correctness of the model. This is true whether I interpret $\\beta_1$ as a conditional treatment effect that is identical across $X$, or as the marginal treatment effect $E(Y\\mid A = 1) - E(Y\\mid A = 0)$.\n\nOn the other hand, when I interpret $\\beta_1$ as the marginal treatment effect and make a statement \n\n$$P(\\beta_1 \\in CI_A \\mid A) = 1 - \\alpha$$\nthen this does not rely on the correctness of the ANCOVA model, in large samples at least (see e.g., [here](https://thestatsgeek.com/2019/04/29/ancova-in-rcts-model-based-standard-errors-are-valid-even-under-misspecification/)).\n\n*I'm taking for granted here that we'd always agree, at a minimum, to assume that $(A_i, X_i, Y_i)$ are i.i.d. random variables with some (possibly unspecified) reasonable joint distribution $f$. We might try to link $f$ to the real world by imagining we take a random sample of patients from an infinite * ***super population*** *and then randomly assign treatment. But of course we do not really take a random sample. And I'm not exactly sure what a * ***super population*** *is.*\n\n# The argument for preferring $CI_{A,X}$ over $CI_A$\n\nThe preceding argument in favour of $CI_A$ seems fairly straightforward. It didn't take me long to write down. To speak in favour of $CI_{A,X}$ requires getting more philosophical.\n\n*This is probably a good point to mention the * [***conditionality principle.***](https://en.wikipedia.org/wiki/Conditionality_principle#:~:text=The%20conditionality%20principle%20is%20a,actually%20performed%20are%20statistically%20irrelevant.)  *Obviously, the type of question I'm asking here is far from new. It has been discussed extensively by many experts starting decades ago, e.g., [here](https://www.jstor.org/stable/2958922), [here](https://www.tandfonline.com/doi/pdf/10.1080/01621459.2000.10474344?casa_token=wd8U_QIuh5sAAAAA:aKTCTl3YqZum4GF9Gt5rUT6tI3jbZ4hlBtGRJVcS2dfk1mfTLC5O_oRpjVQMPD-TFH4K7QOhwceq), [here](https://books.google.ch/books?hl=en&lr=&id=7fz8JGLmWbgC&oi=fnd&pg=PA3&dq=berger+wolpert+1988&ots=iWqqYIhz0_&sig=p0cWQsAt--p_3-SHzrUd0J2RmR0&redir_esc=y#v=onepage&q=berger%20wolpert%201988&f=false) and many more. Even my specific question relative to RCTs has received a lot of attention. [This](https://www.stat.ubc.ca/~john/papers/LiuSIM2009.pdf) is a good example, among many others. I'd highly recommend reading this work rather than anything I'm about to write.* \n\nIn a nutshell, an analyst providing us with a confidence interval $CI_A$ asks us in some sense (which I'll explain with an example below) to *pretend* that we haven't seen $X$. They (the analyst) have seen it and used it, but we (the consumer) have to pretend we haven't in order to claim (with a clear conscience) the robustness property described above.\n\n\n# Example\n\n\nSuppose $N = 600$ and the distribution of $X$ in the super population is $N(0,1)$. Suppose in our particular RCT the observed (A, X) was as follows...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(302)\nx_fixed_1 <- rnorm(600)\na_fixed_1 <- rbinom(600, 1, 0.5)\n\nplot(factor(a_fixed_1), x_fixed_1,\n     xlab = \"Treatment\",\n     ylab = \"X\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNow suppose the outcome model is \n\n$$Y_i \\mid X_i \\sim N(900 + 0 A_i - \\exp(X_i + 4), 60^2)$$\n\nSo the treatment effect, conditional or marginal, is $0$.\n\nWe can plot $Y$ against $X$ for a single realization...\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_ex <- rnorm(600,\n              mean = 900 - exp(x_fixed_1 + 4),\n              sd = 60)\n\nplot(x_fixed_1[a_fixed_1 == 0], y_ex[a_fixed_1 == 0],\n     xlab = \"X\",\n     ylab = \"Y\")\n\npoints(x_fixed_1[a_fixed_1 == 1], y_ex[a_fixed_1 == 1],\n       col = 2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\nUnder this 'true' model, we can check the coverage probability of $CI_{A,X}$...\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_1_fixed <- function(dummy = 0, \n                        x_fixed,\n                        a_fixed){\n  \n  y <- rnorm(600,\n             mean = 900 - exp(x_fixed + 4),\n             sd = 60)\n\n  dat <- data.frame(y = y, x = x_fixed, a = a_fixed)\n  \n  fit <- lm(y ~ a + x, data = dat)\n  \n  ci <- confint(fit)[\"a\",]\n  \n}\n\nci_list_1 <- purrr::map(rep(0,1000), sim_1_fixed, \n                        x_fixed = x_fixed_1,\n                        a_fixed = a_fixed_1)\n\nci_mat_1 <- do.call(rbind, ci_list_1)\n\n# check coverage\nmean(ci_mat_1[,1] < 0 & ci_mat_1[,2] > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.819\n```\n:::\n:::\n\nFar below a nominal 95\\%.\n\nConversely, if instead of conditioning on the observed $(A, X)$, we only condition on the observed $A$, we can confirm that the coverage of $CI_A$ is as expected.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_1_random <- function(dummy = 0, a_fixed){\n  \n  x <- rnorm(600)\n  y <- rnorm(600,\n             mean = 900 - exp(x + 4),\n             sd = 60)\n\n  dat <- data.frame(y = y, x = x, a = a_fixed)\n  \n  fit <- lm(y ~ a + x, data = dat)\n  \n  ci <- confint(fit)[\"a\",]\n  \n}\n\nci_r_list <- purrr::map(rep(0,1000), sim_1_random, a_fixed = a_fixed_1)\nci_r_mat <- do.call(rbind, ci_r_list)\n\n# check coverage\nmean(ci_r_mat[,1] < 0 & ci_r_mat[,2] > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.954\n```\n:::\n:::\n\n\n\nNow, going back to the conditional case, I created $(A, X)$ from a random seed of `302`. If, instead, I create $(A,X)$ with a random seed of `303` and check the conditional coverage probability again, I find that it is more than $95\\%$...\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(303)\nx_fixed_2 <- rnorm(600)\n\nci_list_2 <- purrr::map(rep(0,1000), sim_1_fixed, \n                        x_fixed = x_fixed_2,\n                        a_fixed = a_fixed_1)\n\nci_mat_2 <- do.call(rbind, ci_list_2)\n\n# check coverage\nmean(ci_mat_2[,1] < 0 & ci_mat_2[,2] > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.973\n```\n:::\n:::\n\nSo the $95\\%$ coverage of $CI_A$ is based on a mixture of experiments with different $(A, X)$. For some $(A,X)$, e.g. when the seed is `302`, fewer than 95\\% of experiments will cover $0$, while for some other $(A,X)$, e.g., when the seed is `303`, more than 95\\% of experiments will cover $0$.\n\nIf we can see $(A,X)$, then we know which of these situations we are in. \n\nOften, a betting analogy is used. If I were to use $CI_A$ as a basis to make bets about the true treatment effect, then, in principle at least, someone with access to the data could potentially make easy money from me. Of course, they don't know the true outcome model. But they could take a  look. Maybe they could fit a low-degree spline model, or whatever. \n\nIf access to $(A,X)$ were restricted then this kind of thing wouldn't be possible. That's what I meant above when saying we have to pretend we haven't seen $X$.\n\n## My thoughts on this example\n\nFor me, neither $CI_A$ nor $CI_{A,X}$ feel satisfactory in this example. I guess this just shows that I like to condition, since there is nothing technically wrong with $CI_A$.\n\nMy takeaway is that we need to use a half-decent model. This may conflict with the need to pre-specify a primary analysis method in an RCT and we need to find a pragmatic way through that conflict. I don't think $CI_A$ necessarily saves us from this.\n\n\n\n# Practical consequences\n\nFirst, while in the ANCOVA example described above, the intervals $CI_A$ and $CI_{A,X}$ would be identical, this is not the case in other situations, for example with binary endpoints. In these situations, one needs to make a deliberate choice about which framework to follow. There has been a lot of very impressive work on this recently, for example [here](https://arxiv.org/abs/2302.10404), but I think we need to discuss it a bit more.\n\nSecond, there's the issue of how much importance to place on the robustness property described above for favouring $CI_A$ over $CI_{A,X}$. Do we need to be very strict that our primary analysis methods  give correct coverage conditional on $A$ but not $X$, regardless of model correctness?  For example, I think this [excellent presentation](https://thestatsgeek.com/2023/06/23/is-the-ich-e9-estimand-addendum-compatible-with-model-based-estimands/) by the [Stats Geek](https://thestatsgeek.com/about-thestatsgeek-com/) heads a bit in this direction. (Note: another [great Stats Geek post](https://thestatsgeek.com/2018/11/20/comment-on-conditional-estimation-and-inference-to-address-observed-covariate-imbalance-in-randomized-clinical-trials/) covers similar ground to what I'm discussing here). \n\nI'd try to push back on that a little bit. On the one hand, I agree it's nice to have this robustness property. If we used methods that grossly violated this, then that wouldn't be good. However, for the reasons given in the example above, I don't find it so appealing that I would use it in a strict way to categorise what is and isn't acceptable. \n\nAs a concrete example, take the Cox model. It doesn't matter which estimand one chooses (hazard ratio, difference in RMST, difference in milestone survival, etc.), if a Cox model is used then the type of robustness property described above will never hold. The correctness of the inference, whether conditional or marginal, is dependent on the correctness of the Cox model. \n\nDoes this disqualify the Cox model? I would argue not. We can still probe the robustness of the model ([see my previous post](https://dominicmagirr.github.io/blog/posts/stratify_or_adjust/)) and check for any big problems. Plus, of course, there's the whole area of model checking once we have the data, although this obviously has its own challenges.\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}